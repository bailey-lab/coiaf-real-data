---
title: "Creating Pf6k filtered SNPs for RMCL"
author: "OJ Watson"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: vignette
    toc: true
    fig_caption: yes
editor_options: 
  chunk_output_type: console
---

```{r imports, echo=F, warning=F, message=F, results='hide'}
library(RColorBrewer)
library(here)
library(rslurm)
library(vcfR)
```

This script will:

1. Download the Pf6k VCFs
2. Filter to quality, biallelic SNPs
3. Subset to samples that pass Pf6k inclusion
4. Identify suitable subregions to group samples into 
5. Identify loci that have MAF > 0.01 globally and MAF > 0.05 regionally
6. Filter to sets of loci that are not in linkage disequilbirium
7. Resultant loci are converted into homo/het calls
8. Homo/het calls for each region are sent to RMCL for COI
9. Summary estimates of COI are estimated for each sample

## Locations to download and filter

```{r define directories}
# Directory which contains the sub directories
home_dir <- "/gpfs/data/jbailey5/apascha1/"

# Directory where we will do our filtering and subsetting etc
filter_dir <- paste0(home_dir, "pf_release6_COI_filtering")

# Directory where we will download the Pf 6 samples to
sample_dir <- paste0(home_dir, "pf_release6")

# Directory for slurm outputs
slurm_dir <- paste0(home_dir, "slurm_outs")
```

## Download the files

```{r imports, echo=F, warning=F, message=F, results='hide'}
ftp_dir <- "ftp://ngs.sanger.ac.uk/production/malaria/pfcommunityproject/Pf6/Pf_6_vcf"

# Grab the file names from the ftp address
filenames <- RCurl::getURL(
  ftp_dir, 
  ftp.use.epsv = FALSE, 
  dirlistonly = TRUE
) %>%
  strsplit("\n") %>%
  unlist()

# Create urls and download to sample directory
urls <- file.path(ftp_dir, filenames)

dir.create(sample_dir, recursive = TRUE)
setwd(sample_dir)
system(paste0("wget -r -nH --cut-dirs=7 ", ftp_dir))
setwd(home_dir)
```

## Filter the vcfs

Filter the Pf6k samples with the following filters used in Zhu et al. [The
origins and relatedness structure of mixed infections vary with local prevalence
of P. falciparum malaria ][https://elifesciences.org/articles/40845].

```{r}
bcf_filter <- function(x, dir) {
  if (x < 10) {
    x <- paste0("0", x)
  }

  l <- list.files(dir, full.names = TRUE)
  s <- grep(paste0("Pf3D7_", x, "_.*vcf.gz$"), l, value = TRUE)
  t <- gsub("pf_release6", "pf_release6_filter_zhu_elife", s)
  dir.create(dirname(t), recursive = TRUE, showWarnings = FALSE)

  # filter to sites that include the expression FILTER="PASS", and to sites that
  # are biallelic (at least 2 alleles (-m 2) and a maximum of 2 (-M 2)) and are
  # SNPs
  cmd <- paste0("module load bcftools/1.9; bcftools view -i 'FILTER=\"PASS\"' -m 2 -M 2 -v snps -O z \"", s, "\" > \"", t, "\"")
  system(command = cmd)
}

# submit to slurm
pars <- data.frame("x" = 1:14, "dir" = sample_dir)
sopt <- list(time = "1:00:00")
setwd(slurm_dir)

sjob <- rslurm::slurm_apply(bcf_filter, pars,
  jobname = "zhu_filter",
  nodes = 14, cpus_per_node = 1, slurm_options = sopt,
  submit = TRUE
)
rslurm::get_job_status(sjob)
res <- rslurm::get_slurm_out(sjob)
```

## Drop samples that don't pass quality control 

The samples in Pf7k are annotated in terms of whether they were included for
further analysis or not. If they were not included it is usually due to poor
read coverage or being lab samples We'll remove these before moving on.

```{r}
# get the Pf6k sample meta
tf <- tempfile()
download.file("ftp://ngs.sanger.ac.uk/production/malaria/pfcommunityproject/Pf6/Pf_6_samples.txt", tf)
meta <- data.table::fread(tf)

# samples to keep
samples <- meta$Sample[meta$`Exclusion reason` == "Analysis_set"]

# bcftools function for ease
bcft <- function(query, file, outfile = NULL, cmd_only = FALSE) {
  m <- "module load bcftools/1.9;"

  # write to outfile or otherwrite original
  if (!is.null(outfile)) {
    file <- paste(file, ">", outfile)
  } else {
    tf <- tempfile()
    outfile <- tf
    file <- paste(file, ">", outfile)
  }

  cmd <- paste(m, "bcftools", paste(query, collapse = " "), file)

  if (cmd_only) {
    return(cmd)
  } else {
    system(cmd)
    return(outfile)
  }
}

# wrapper for filtering by samples
bcf_sample_subset <- function(samples, file, outfile, cmd_only) {
  tf <- tempfile()
  writeLines(samples, tf)

  bcft(
    query = c("view -O z -S", tf),
    file = file,
    outfile = outfile,
    cmd_only = cmd_only
  )
}

# what files are we working with
l <- list.files(
  paste0(home_dir, "pf_release6_filter_zhu_elife"), 
  full.names = TRUE
)
l <- grep("final.vcf.gz", l, fixed = TRUE, value = TRUE)

# we are going to a series of filters that don't take long to run so let's
# create a new directory to store these in and we will overwrite each time now
# to save space
dir.create(filter_dir)

# build out parameter list
obj_list <- list()
for (i in seq_along(l)) {
  obj_list[[i]] <- list(
    samples = samples,
    file = l[i],
    outfile = file.path(filter_dir, basename(l[i])),
    cmd_only = FALSE
  )
}

sopt <- list(time = "4:00:00", mem = "16Gb")
setwd(slurm_dir)
sjob <- rslurm::slurm_apply(
  f = function(i) {
    bcf_sample_subset(
      samples = obj_list[[i]]$samples,
      file = obj_list[[i]]$file,
      outfile = obj_list[[i]]$outfile,
      cmd_only = obj_list[[i]]$cmd_only
    )
  },
  params = data.frame(i = seq_along(obj_list)),
  global_objects = c("bcf_sample_subset", "bcft", "obj_list"),
  jobname = "exclusion_drops",
  nodes = 14,
  cpus_per_node = 1,
  slurm_options = sopt,
  submit = TRUE
)

rslurm::get_job_status(sjob)
res <- rslurm::get_slurm_out(sjob)
```

## Select intermediate frequency and high quality SNPs

We'll continue by subsetting to the SNPs that have a minor allele frequency
greater than 0.01.

```{r}
# frequency filter
bcf_allele_frequency <- function(vcf, minor = 0.01) {
  bcft(paste0("view -O z -q ", minor, ":alt1 "), vcf)
}

# files in out new directory
l <- list.files(filter_dir, full.names = TRUE)
l <- grep("final.vcf.gz", l, fixed = TRUE, value = TRUE)

# submit filter
pars <- data.frame("vcf" = l, "minor" = 0.01, stringsAsFactors = FALSE)
sopt <- list(time = "4:00:00", mem = "16Gb")

setwd(slurm_dir)
sjob <- rslurm::slurm_apply(bcf_allele_frequency,
  pars,
  jobname = "maf_allele_filter",
  nodes = 14,
  global_objects = "bcft",
  cpus_per_node = 1,
  slurm_options = sopt,
  submit = TRUE
)
rslurm::get_job_status(sjob)
res <- rslurm::get_slurm_out(sjob)
```

Next let's look to see how many variants this has returned for us:

```{r}
# grab the files we are working with
l <- list.files(filter_dir, full.names = TRUE)
tfvars <- gsub(
  ".vcf.gz", 
  ".loci", 
  grep(".vcf.gz", l, fixed = TRUE, value = TRUE), 
  fixed = TRUE
)

# calculate the number of positions
for (i in seq_along(tfvars)) {
  if (!file.exists(tfvars[i])) {
    system(paste(
      "module load bcftools/1.9; bcftools query -f '%POS\n' ", 
      l[i], 
      ">", 
      tfvars[i]
    ))
  }
}

# how many per chromosome
num_variants <- lapply(tfvars, function(x) {
  as.numeric(system(paste("cat", x, "| wc -l"), intern = TRUE))
})

# create these into a data frame
variants <- lapply(seq_along(tfvars), function(x) {
  df <- data.table::fread(tfvars[x])
  df$chrom <- x
  names(df)[1] <- "pos"
  df$y <- 1
  return(df)
})

df <- data.table::rbindlist(variants)

## plot their locations
plot_locations <- function(df) {
  Pf_chrom_lengths <- function() {
    ret <- data.frame(
      chrom = 1:14,
      length = c(
        643292, 947102, 1060087,
        1204112, 1343552, 1418244,
        1501717, 1419563, 1541723,
        1687655, 2038337, 2271478,
        2895605, 3291871
      )
    )
    return(ret)
  }

  df_chrom_lengths <- Pf_chrom_lengths()

  df_vlines <- df_chrom_lengths[rep(1:14, each = 16), ]
  df_vlines$x <- rep(1:16 * 2e5 + 1, times = 14)
  df_vlines <- subset(df_vlines, df_vlines$x < df_vlines$length)

  # produce basic plot
  plot1 <- ggplot(df) +
    facet_wrap(~chrom, ncol = 1)
  plot1 <- plot1 + theme(
    strip.background = element_blank(),
    strip.text.x = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank()
  )

  # add rectangles and grid lines
  plot1 <- plot1 + geom_rect(
    aes(xmin = 1, xmax = length, ymin = 0, ymax = 1), 
    col = grey(0.7), 
    size = 0.2, 
    fill = grey(0.95), 
    data = df_chrom_lengths
  )
  plot1 <- plot1 + 
    geom_segment(
      aes(x = x, y = 0, xend = x, yend = 1), 
      col = grey(0.8), 
      size = 0.1, 
      data = df_vlines
    )

  # set y scale
  plot1 <- plot1 + theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.title.y = element_blank()
  )

  # add data
  plot1 <- plot1 + geom_segment(aes(x = pos, y = 0, xend = pos, yend = y))

  # labels and legends
  x11()
  plot1 <- plot1 + xlab("position")
  print(plot1)
  invisible(plot1)
}

ggplot2::ggsave(
  paste0(home_dir, "plots/chrom_locations.png"),
  plot_locations(df)
)
```

## Subset SNPs to those that pass at regional level

The SNPs we have chosen have suitable MAF across all samples, which is global.
We will now subset further to those that remain when we repeat the filtering
within chosen demes.

Deciding how many "demes" there effectively are is difficult. One approach is to
look at the genetics and look at Fst between chosen demes.

Alternatively, we could look at the number of clusters based on k means
clustering and a silhouette based assessment of the correct number of clusters.

```{r}
# get the Pf6k sample meta
tf <- tempfile()
download.file("ftp://ngs.sanger.ac.uk/production/malaria/pfcommunityproject/Pf6/Pf_6_samples.txt", tf)
meta <- data.table::fread(tf)

# do k means clustering
library(cluster)
locations <- unique(meta[, c("Lat", "Long")])

sis <- 2:(nrow(locations) - 1)
for (k in sis) {
  si <- silhouette(cluster::pam(x = locations, k))
  sis[k - 1] <- mean(si[, 3])
}

plot(sis)
abline(v = which(diff(sign(diff(sis))) == -2) + 1)
text(
  which(diff(sign(diff(sis))) == -2) + 2, 
  0.1, 
  which(diff(sign(diff(sis))) == -2) + 2
)
```

Looking at the silhoutte plot is not super clear and there are a few contenders.
Given that we can be confident there are more than than 5 clusters in the world
with significant differences in PLAF, then any of 13, 18, 24, 27, 29 or 33 seem
all suitable. We could plot what these look like to see which seems sensible:

```{r, eval = FALSE}
# This works as an aside
install.packages("sf",
  configure.args =
    "--with-proj-include=/gpfs/runtime/opt/proj/5.2.0/bin/
--with-proj-lib=/gpfs/runtime/opt/proj/5.2.0/lib
--with-proj-share=/gpfs/runtime/opt/proj/5.2.0/share/proj"
)

# This struggles...
install.packages("rgdal",
  configure.args =
    "--with-proj-include=/gpfs/runtime/opt/proj/5.2.0/bin/
--with-proj-lib=/gpfs/runtime/opt/proj/5.2.0/lib
--with-proj-share=/gpfs/runtime/opt/proj/5.2.0/share"
)

df <- meta[meta$`Exclusion reason` == "Analysis_set", c("Lat", "Long")]
ks <- cluster::pam(df[, 1:2], k = 24)
df$color <- as.factor(ks$clustering)
roxer::map_plot(df, "Lat", "Long", "text")
```

24 seems suitable. So let's break the sites down into 24 and then collect the
suitable loci and check them agianst those previously identified.

How many samples is that per site? 

```{r}
table(df$color)
```

Okay so a few sites are underrepresented (18 and 19) so we may need to exclude
these when identifying the intersect of the global SNPs and regional SNPs.

```{r}
meta <- meta[meta$`Exclusion reason` == "Analysis_set", ]
ks <- cluster::pam(meta[, c("Lat", "Long")], k = 24)
meta$k_cluster <- as.factor(ks$clustering)

# funtion to subset by samples and then count the variant
sample_maf_subset_count <- function(samples, minor = 0.05, vcf) {
  tf_s <- tempfile()
  tf <- tempfile()
  writeLines(samples, tf_s)

  system(paste("module load bcftools/1.9;",
    "bcftools view -S ",
    tf_s, vcf,
    paste0("| bcftools view -q ", minor, ":alt1", collapse = ""),
    "| bcftools query -f '%POS\n' ",
    "> ", tf
  ))

  return(readLines(tf))
}

# build parameter obj list
l <- list.files(filter_dir, full.names = TRUE)
l <- grep("final.vcf.gz", l, value = TRUE)

obj_list <- vector(
  mode = "list", 
  length = length(unique(meta$k_cluster)) * length(l)
)
count <- 1
for (i in seq_len(length(unique(meta$k_cluster)))) {
  samples <- meta$Sample[meta$k_cluster == i]
  for (j in seq_len(length(l))) {
    obj_list[[count]]$samples <- samples
    obj_list[[count]]$vcf <- l[j]
    obj_list[[count]]$minor <- 0.05
    count <- count + 1
  }
}

# submit filter
setwd(slurm_dir)
sopt <- list(time = "4:00:00", mem = "16Gb")

sjob <- rslurm::slurm_apply(
  f = function(i) {
    sample_maf_subset_count(
      samples = obj_list[[i]]$samples,
      minor = obj_list[[i]]$minor,
      vcf = obj_list[[i]]$vcf
    )
  },
  params = data.frame(i = seq_along(obj_list)),
  global_objects = c("sample_maf_subset_count", "obj_list"),
  jobname = "kmeans_filters",
  nodes = length(obj_list),
  cpus_per_node = 1,
  slurm_options = sopt,
  submit = TRUE
)

rslurm::get_job_status(sjob)
res <- rslurm::get_slurm_out(sjob)
```

Now processing to calculate the intercepting SNPs between regionally MAF>0.01
and global MAF>0.01

```{r}
# what are the samples and vcfs we are working with
samples <- lapply(obj_list, "[[", "samples")
vcf <- lapply(obj_list, "[[", "vcf")
intersects <- vector("list", length(l))
k <- length(unique(meta$k_cluster))

# loop through the vcfs and see which loci are common across n regions up to all
# regions
for (i in seq_len(length(l))) {
  pos <- which(unlist(lapply(vcf, identical, y = vcf[[i]])))
  count <- table(as.numeric(unlist(res[pos])))
  for (j in 1:24) {
    intersects[[i]][[j]] <- names(which(count >= (j)))
  }
}

# plot the fall of loci with regions
inclusions <- data.frame(
  "regions" = 1:24,
  "loci" = sapply(1:24, function(x) {
    length(unlist(lapply(intersects, "[[", x)))
  })
)
inclusions_plot <- ggplot(inclusions, aes(x = regions, y = loci)) +
  geom_point() +
  geom_line()
ggplot2::ggsave(
  paste0(home_dir, "plots/inclusions.png"), 
  inclusions_plot
)

# is it the same region screwing up as we go between the loci found in 24
# regions vs 23 regions
missing <- lapply(1:14, function(x) {
  setdiff(intersects[[x]][[23]], intersects[[x]][[24]])
})

found <- vector("list", length(l))
for (i in seq_len(length(l))) {
  pos <- which(unlist(lapply(vcf, identical, y = vcf[[i]])))
  for (j in seq_along(pos)) {
    found[[i]][[j]] <- sum((is.na(match(missing[[i]], res[[pos[j]]]))))
  }
}

do.call(rbind, found)
```

So as suspected, region 19 is contributing most of the spurious SNPs (region had
only 37 samples) and then region 15 (80 samples). So most likely these are not
useful SNPs maintained globally and could either be driven by low N or because
they are geographically different.

So we will focus on the intersect between the SNPs identified in all regions and
the global ones:

```{r}
# regional SNPs
regional_snps <- lapply(intersects, "[[", 24)

# global SNPs
l <- list.files(new_dir, full.names = TRUE)
loci <- tfvars <- grep("loci", l, value = TRUE)
global_snps <- lapply(loci, readLines)

# intersect
intersect_snps <- mapply(intersect, regional_snps, global_snps)
saveRDS(intersect_snps, paste0(home_dir, "derived/intercept_snps_0.01.rds"))
```

## Subsetting to intersecting SNPs

```{r}
# filter by loci
loci_subset <- function(loci, vcf, chrom, vcfout) {

  # create our loci filter table
  tf <- tempfile(fileext = ".txt")
  loc_df <- data.frame(
    "CHROM" = paste0("Pf3D7_", chrom, "_v3"),
    "POS" = loci
  )
  write.table(
    loc_df, 
    tf, 
    row.names = FALSE, 
    quote = FALSE, 
    sep = "\t", 
    col.names = FALSE
  )

  # filter our vcf
  system(paste("module load bcftools/1.9; bcftools view -O z -T", tf, vcf, ">", vcfout))
}

# samples to be filtered
intersect_snps <- readRDS(paste0(home_dir, "snp_selections/intersect_snps_0.005.rds"))

l <- list.files(filter_dir, full.names = TRUE)
l <- grep("vcf.gz", l, value = TRUE, fixed = TRUE)

# create obj list
obj_list <- vector("list", 14)
for (i in seq_along(obj_list)) {
  obj_list[[i]]$loci <- intersect_snps[[i]]
  obj_list[[i]]$vcf <- l[i]
  obj_list[[i]]$chrom <- ifelse(nchar(i) == 1, paste0("0", i), i)
  obj_list[[i]]$vcfout <- gsub("final", "final_coi", l[i])
}

# submit filter
setwd(slurm_dir)
sopt <- list(time = "4:00:00", mem = "16Gb")

# submit jobs
sjob <- rslurm::slurm_apply(
  f = function(i) {
    loci_subset(
      loci = obj_list[[i]]$loci,
      vcf = obj_list[[i]]$vcf,
      chrom = obj_list[[i]]$chrom,
      vcfout = obj_list[[i]]$vcfout
    )
  },
  params = data.frame(i = seq_along(obj_list)),
  global_objects = c("loci_subset", "obj_list"),
  jobname = "loci_filters",
  nodes = length(obj_list),
  cpus_per_node = 1,
  slurm_options = sopt,
  submit = TRUE
)

rslurm::get_job_status(sjob)
res <- rslurm::get_slurm_out(sjob)
```

## Subsampling to sites that are not in LD

We will need to choose loci that are not in LD. This will also help bring down
the number of loci to be analysed, which will help THE REAL McCOIL out a lot.

We can start though by merging our individual chromosome VCFs into one, from
which we will conduct LD filtering

```{r}
# get the VCFs
l <- list.files(filter_dir, full.names = TRUE)
l <- grep("final_coi", l, value = TRUE)

# output file
O <- gsub("_01_v3", "", l[1])

# loop through and get fix and gt
fix <- vector("list", length(l))
gt <- vector("list", length(l))
for (i in 1:length(l)) {
  vcfRobj_i <- vcfR::read.vcfR(l[i])
  fix[[i]] <- vcfRobj_i@fix
  gt[[i]] <- vcfRobj_i@gt
}

# append all together and write to file
meta <- append(vcfRobj_i@meta, "##Combined chromosomes into one VCF")
fixs <- do.call(rbind, fix)
gts <- do.call(rbind, gt)
newvcfR <- new("vcfR", meta = meta, fix = fixs, gt = gts)

## write the vcf to file as well as the rds
vcfR::write.vcf(newvcfR, O)
vcf_rds <- paste0(home_dir, "snp_selections/vcf_final.rds")
saveRDS(newvcfR, vcf_rds)
```

We can now filter to create subsets that are not in LD. 

```{r}
# Get the VCFs
l <- list.files(filter_dir, full.names = TRUE)
l <- grep("final_coi", l, value = TRUE)

# Output file
O <- gsub("_01_v3", "", l[1])

# read in our final VCF
vcfRobj <- vcfR::read.vcfR(O)

# calculate autocorrelation
gen <- vcfRmanip::genautocorr(vcfR = vcfRobj)
gen_rds <- paste0(home_dir, "snp_selections/gen.rds")
saveRDS(gen, gen_rds)

# Read rds file paths
gen_rds <- paste0(home_dir, "snp_selections/gen.rds")
vcf_rds <- paste0(home_dir, "snp_selections/vcf_final.rds")

# Create subsets
# --------------------------
# LD filter wrapped
ld_filter <- function(vcfRobj_rds, genauto_rds, threshR2 = 0.8, random = TRUE) {
  vcfRobj <- readRDS(vcfRobj_rds)
  gen <- readRDS(genauto_rds)
  vcfRmanip::vcfR2LDfiltered(
    vcfR = vcfRobj, genautocorrresult = gen,
    threshR2 = threshR2, random = random
  )
}

# parameter df
pars <- data.frame(
  "vcfRobj_rds" = vcf_rds,
  "genauto_rds" = gen_rds,
  "threshR2" = rep(0.8, 10),
  "random" = TRUE,
  stringsAsFactors = FALSE
)

# submit ld filter
setwd(slurm_dir)
sopt <- list(time = "4:00:00", mem = "32Gb")

sjob <- rslurm::slurm_apply(ld_filter,
  pars,
  jobname = "ld_filter",
  nodes = 10,
  global_objects = "ld_filter",
  cpus_per_node = 1,
  slurm_options = sopt,
  submit = TRUE
)
rslurm::get_job_status(sjob)
res <- rslurm::get_slurm_out(sjob)
```


## Running RMCL

Now that we have suitable datasets we need to split them up into their regions
and then create our heterozygous calls for putting into RMCL

```{r}
# function to load vcf and subset to samples in given region, create gt and then RMCL
submit_rmcl <- function(rep, vcf_rds, region, meta_df, path) {

  # read in the vcf
  vcfRobj <- readRDS(vcf_rds)
  if (is.list(vcfRobj)) {
    vcfRobj <- vcfRobj[[1]]
  }

  vcf_num <- gsub(".*(\\d)\\.RDS", "\\1", basename(vcf_rds))

  # subset to samples for this region
  vcfRobj <- vcfRmanip::select_samples(
    vcfRobj, 
    meta_df$Sample[meta_df$cluster == region]
  )

  # create genotype matrix for RMCL (-1, 0, 0.5, 1)
  gtmat <- vcfRmanip::gtmat012(vcfRobj) / 2
  gtmat[is.na(gtmat)] <- -1
  gtmat <- t(gtmat)
  colnames(gtmat) <- rownames(vcfRobj@fix)

  # create output file name
  output <- paste0("cat_region_", region, "_vcf_", vcf_num, "_rep_", rep, ".txt")

  # quick log
  message("Running RMCL")

  # Run RMCL categorical
  out <- McCOILR::McCOIL_categorical(
    data = gtmat,
    maxCOI = 25,
    totalrun = 100000,
    burnin = 1000,
    M0 = 5,
    threshold_ind = round(ncol(gtmat) * 0.25),
    threshold_site = round(nrow(gtmat) * 0.20),
    thin = 0.01,
    err_method = 3,
    path = path,
    output = output
  )

  # remove the trace for file size reasons
  file.path(path, output)
  return(out)
}


# recreate our data frame for the regional clustering

## get the Pf6k sample meta
tf <- tempfile()
download.file("ftp://ngs.sanger.ac.uk/production/malaria/pfcommunityproject/Pf6/Pf_6_samples.txt", tf)
meta <- data.table::fread(tf)

## assign to clusters
df <- meta[meta$`Exclusion reason` == "Analysis_set", c("Lat", "Long", "Sample")]
ks <- cluster::pam(df[, 1:2], k = 24)
df$cluster <- as.factor(ks$clustering)

# create our source vcf paths
vcfs <- grep("results_",
  list.files(paste0(slurm_dir, "rslurm_ld_filter"), full.names = TRUE),
  value = TRUE
)

# build our parameter object list
obj_grid <- expand.grid(rep = 1:5, vcfs = vcfs, regions = unique(df$cluster), stringsAsFactors = FALSE)
obj_list <- vector("list", nrow(obj_grid))
for (i in seq_along(obj_list)) {
  obj_list[[i]]$rep <- obj_grid$rep[i]
  obj_list[[i]]$vcf_rds <- obj_grid$vcfs[i]
  obj_list[[i]]$region <- obj_grid$regions[i]
  obj_list[[i]]$meta_df <- df
  obj_list[[i]]$path <- file.path(here::here(), "analysis/data/derived/McCOIL_work")
}


# submit filter
setwd(slurm_dir)
sopt <- list(time = "24:00:00", mem = "16Gb")

# submit jobs
sjob <- rslurm::slurm_apply(
  f = function(i) {
    submit_rmcl(
      rep = obj_list[[i]]$rep,
      vcf_rds = obj_list[[i]]$vcf_rds,
      region = obj_list[[i]]$region,
      meta_df = obj_list[[i]]$meta_df,
      path = obj_list[[i]]$path
    )
  },
  params = data.frame(i = seq_along(obj_list)),
  global_objects = c("submit_rmcl", "obj_list"),
  jobname = "RMCL",
  nodes = length(obj_list),
  cpus_per_node = 1,
  slurm_options = sopt,
  submit = FALSE
)
rslurm::get_job_status(sjob)
res <- rslurm::get_slurm_out(sjob)
```
